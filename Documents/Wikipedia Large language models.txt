Source:
https://en.wikipedia.org/wiki/Wikipedia:Large_language_models
Subject:
Wikipedia:Large language models
Content:
From Wikipedia, the free encyclopedia
"WP:CHATGPT" redirects here. For the reliability of ChatGPT, see WP:RSPCHATGPT.
	
This is an essay.
It contains the advice or opinions of one or more Wikipedia contributors. This page is not an encyclopedia article, nor is it one of Wikipedia's policies or guidelines, as it has not been thoroughly vetted by the community. Some essays represent widespread norms; others only represent minority viewpoints.
Shortcuts
WP:LLM
WP:LLMS
WP:GPT
WP:CHATBOT
WP:CHATGPT
	This page in a nutshell: Avoid using large language models (LLMs) to write original content, generate references, or create replies on discussion pages. LLMs can be used for certain tasks (like copyediting) if the editor has substantial prior experience in the intended task and rigorously scrutinizes the results before publishing.
“	Large language models have limited reliability, limited understanding, limited range, and hence need human supervision.	”
— Michael Osborne, Professor of Machine Learning, University of Oxford[1]
While large language models (colloquially termed "AI chatbots" in some contexts) can be very useful, machine-generated text—much like human-created text—can contain errors or flaws, or be outright useless.

Specifically, asking an LLM to "write a Wikipedia article" can sometimes cause the output to be outright fabrication, complete with fictitious references. It may base itself on bias, may libel living people, or may violate copyrights. Thus, all text generated by LLMs should be verified by editors before use in articles. The same applies to edits using references generated largely or fully by an LLM, for which editors must use other sources instead.

Editors who are not fully aware of these risks and not able to overcome the limitations of these tools should not edit with their assistance. LLMs should not be used for tasks with which the editor does not have substantial familiarity. Their outputs should be rigorously scrutinized for compliance with all applicable policies. In any case, editors should avoid publishing content on Wikipedia obtained by asking LLMs to write original content. Even if such content has been heavily edited, alternatives that do not use machine-generated content are preferable. As with all edits, an editor is fully responsible for their LLM-assisted edits.

Wikipedia is not a testing ground. Using LLMs to write one's talk page comments or edit summaries, in a non-transparent way, is strongly discouraged. LLMs used to generate or modify text should be mentioned in the edit summary, even if their terms of service do not require it.

Risks and relevant policies
Shortcuts
WP:AIFAIL
WP:PADEMELONS
Original research and "hallucinations"
For the relevant policy, see Wikipedia:No original research.
Wikipedia articles must not contain original research – i.e. facts, allegations, and ideas for which no reliable, published sources exist. This includes any analysis or synthesis of published material that serves to reach or imply a conclusion not stated by the sources. To demonstrate that you are not adding original research, you must be able to cite reliable, published sources. They should be directly related to the topic of the article and directly support the material being presented.
LLMs are pattern completion programs: They generate text by outputting the words most likely to come after the previous ones. They learn these patterns from their training data, which includes a wide variety of content from the Internet and elsewhere, including works of fiction, low-effort forum posts, unstructured and low-quality content for search engine optimization (SEO), and so on. Because of this, LLMs will sometimes "draw conclusions" which, even if they seem superficially familiar, are not present in any single reliable source. They can also comply with prompts with absurd premises, like "The following is an article about the benefits of eating crushed glass". Finally, LLMs can make things up, which is a statistically inevitable byproduct of their design, called "hallucination". All of this is, in practical terms, equivalent to original research, or worse, outright fabrication.

“	In short, the "hallucinations" and biases in generative AI outputs result from the nature of their training data, the tools’ design focus on pattern-based content generation, and the inherent limitations of AI technology...	”
— In: "When AI Gets It Wrong", MIT Sloan School of Management, AI basics[2]
As LLMs often output accurate statements, and since their outputs are typically plausible-sounding and given with an air of confidence, any time that they deliver a useful-seeming result, people may have difficulty detecting the above problems. An average user who believes that they are in possession of a useful tool, who maybe did a spot check for accuracy and "didn't see any problems", is biased to accept the output as provided; but it is highly likely that there are problems. Even if 90% of the content is okay and 10% is false, that is a huge problem in an encyclopedia. LLMs' outputs become worse when they are asked questions that are complicated, about obscure subjects, or told to do tasks to which they are not suited (e.g. tasks which require extensive knowledge or analysis). An example of this tendency is shown below:

Prompt:

Greater-toothed pademelons, Thylogale gigadonti, can be seen in wildlife preserves in Queensland

Note that "greater-toothed pademelon" is a fictitious species.

Response:

Yes, greater-toothed pademelons are found in wildlife preserves in Queensland. They are shy and nocturnal, so it may be difficult to see them during the day. However, you may be able to see their tracks or hear them rustling in the bushes at night

This complete statement is false and is based on the LLM's assumption that this was a real species. In order to provide the most plausible answer, it extracted general information about a different kind of pademelon (shown in underline) from an Australian Museum article. This serves to demonstrate that LLM's can offer statements with a confident tone even when that information is factually incorrect or unverifiable.

(LLM Used: Gemini)

Unsourced or unverifiable content
For the relevant policy, see Wikipedia:Verifiability.
Readers must be able to check that any of the information within Wikipedia articles is not just made up. This means all material must be attributable to reliable, published sources. Additionally, quotations and any material challenged or likely to be challenged must be supported by inline citations
LLMs do not follow Wikipedia's policies on verifiability and reliable sourcing. LLMs sometimes exclude citations altogether or cite sources that don't meet Wikipedia's reliability standards (including citing Wikipedia as a source). In some cases, they hallucinate citations of non-existent references by making up titles, authors, and URLs.

LLM-hallucinated content, in addition to being original research as explained above, also breaks the verifiability policy, as it can't be verified because it is made up: there are no references to find.

Algorithmic bias and non-neutral point of view
For the relevant policy, see Wikipedia:Neutral point of view.
Articles must not take sides, but should explain the sides, fairly and without editorial bias. This applies to both what you say and how you say it.
LLMs can produce content that is neutral-seeming in tone, but not necessarily in substance. This concern is especially salient for biographies of living persons.

Copyright violations
For the relevant policy, see Wikipedia:Copyrights. Further information: Wikipedia:Large language models and copyright
If you want to import text that you have found elsewhere or that you have co-authored with others (including LLMs), you can only do so if it is available under terms that are compatible with the CC BY-SA license.
Examples of copyright violations by LLMs at 2:00

Slides for examples of copyright violations by LLMs
LLMs can generate material that violates copyright.[a] Generated text may include verbatim snippets from non-free content or be a derivative work. In addition, using LLMs to summarize copyrighted content (like news articles) may produce excessively close paraphrases.

The copyright status of LLMs trained on copyrighted material is not yet fully understood. Their output may not be compatible with the CC BY-SA license and the GNU license used for text published on Wikipedia.

Usage
Wikipedia relies on volunteer efforts to review new content for compliance with our core content policies. This is often time consuming. The informal social contract on Wikipedia is that editors will put significant effort into their contributions, so that other editors do not need to "clean up after them". Editors should ensure that their LLM-assisted edits are a net positive to the encyclopedia, and do not increase the maintenance burden on other volunteers.

Specific competence is required
Shortcut
WP:LLMCIR
LLMs are assistive tools, and cannot replace human judgment. Careful judgment is needed to determine whether such tools fit a given purpose. Editors using LLMs are expected to familiarize themselves with a given LLM's inherent limitations and then must overcome these limitations, to ensure that their edits comply with relevant guidelines and policies. To this end, prior to using an LLM, editors should have gained substantial experience doing the same or a more advanced task without LLM assistance.[b]

Some editors are competent at making unassisted edits but repeatedly make inappropriate LLM-assisted edits despite a sincere effort to contribute. Such editors are assumed to lack competence in this specific sense. They may be unaware of the risks and inherent limitations or be aware but not be able to overcome them to ensure policy-compliance. In such a case, an editor may be banned from aiding themselves with such tools (i.e., restricted to only making unassisted edits). This is a specific type of limited ban. Alternatively, or in addition, they may be partially blocked from a certain namespace or namespaces.

Disclosure
Shortcut
WP:LLMDISCLOSE
Every edit that incorporates LLM output should be marked as LLM-assisted by identifying the name and, if possible, version of the AI in the edit summary. This applies to all namespaces.

Writing articles
Shortcut
WP:LLMWRITE
Pasting raw large language models' outputs directly into the editing window to create a new article or add substantial new prose to existing articles generally leads to poor results. LLMs can be used to copyedit or expand existing text and to generate ideas for new or existing articles. Every change to an article must comply with all applicable policies and guidelines. This means that the editor must become familiar with the sourcing landscape for the topic in question and then carefully evaluate the text for its neutrality in general, and verifiability with respect to cited sources. If citations are generated as part of the output, they must verify that the corresponding sources are non-fictitious, reliable, relevant, and suitable sources, and check for text–source integrity.

If using an LLM as a writing advisor, i.e. asking for outlines, how to improve paragraphs, criticism of text, etc., editors should remain aware that the information it gives is unreliable. If using an LLM for copyediting, summarization, and paraphrasing, editors should remain aware that it may not properly detect grammatical errors, interpret syntactic ambiguities, or keep key information intact. It is possible to ask the LLM to correct deficiencies in its own output, such as missing information in a summary or an unencyclopedic, e.g., promotional, tone, and while these could be worthwhile attempts, they should not be relied on in place of manual corrections. The output may need to be heavily edited or scrapped. Due diligence and common sense are required when choosing whether to incorporate the suggestions and changes.

Raw LLM outputs should not be added directly into drafts either. Drafts are works in progress and their initial versions often fall short of the standard required for articles, but enabling editors to develop article content by starting from an unaltered LLM-outputted initial version is not one of the purposes of draft space or user space.

Communicating
Shortcut
WP:LLMTALK
Editors should not use LLMs to write comments generatively. Communication is at the root of Wikipedia's decision-making process and it is presumed that editors contributing to the English-language Wikipedia possess the ability to come up with their own ideas. Comments that do not represent an actual person's thoughts are not useful in discussions, and comments that are obviously generated by an LLM or similar AI technology may be struck or collapsed. Repeating such misuse forms a pattern of disruptive editing, and may lead to a block or ban. This does not apply to using LLMs to refine the expression of one's authentic ideas.

Other policy considerations
LLMs should not be used for unapproved bot-like editing (WP:MEATBOT), or anything even approaching bot-like editing. Using LLMs to assist high-speed editing in article space has a high chance of failing the standards of responsible use due to the difficulty in rigorously scrutinizing content for compliance with all applicable policies.

Wikipedia is not a testing ground for LLM development, for example, by running experiments or trials on Wikipedia for this sole purpose. Edits to Wikipedia are made to advance the encyclopedia, not a technology. This is not meant to prohibit editors from responsibly experimenting with LLMs in their userspace for the purposes of improving Wikipedia.

Sources with LLM-generated text
See also: WP:RSPCHATGPT and Generative artificial intelligence § Misuse in journalism
LLM-created works are not reliable sources. Unless their outputs were published by reliable outlets with rigorous oversight, and unless it can be verified that the content were evaluated for accuracy by the publisher, they should not be cited.

Identifying LLM-generated text
Further information: Wikipedia:Signs of AI writing
While it is not always possible to determine whether text is LLM-generated, LLM outputs may sometimes exhibit characteristics that allow readers to tell them apart from human-generated content. For example, a verbose and information-dense talk page comment that is written in an impersonal tone with correct spelling and grammar, yet contains non-wikitext markup and lacks links or citations, is likely to be LLM-generated.

Do not solely rely on artificial intelligence content detection tools (such as GPTZero) to evaluate whether text is LLM-generated, as these tools are unreliable due to their high error rates.

User scripts like WP:UPSD can help identify sections of articles that may have been generated by LLMs.

Handling suspected LLM-generated content
See also: Wikipedia:Content removal
An editor who identifies LLM-originated content that does not comply with our core content policies—and decides not to remove it outright (which is generally fine to do)—should either edit it to make it comply or alert other editors of the issue. The first thing to check is that the referenced works actually exist. All factual claims then need to be verified against the provided sources. Presence of text‑source integrity must be established. Anything that turns out not to comply with the policies should then be removed.

To alert other editors, the editor who responds to the issue should place {{AI-generated|date=August 2025}} at the top of the affected article or draft (only if that editor does not feel capable of quickly resolving the issue on their own). In biographies of living persons, non-policy compliant LLM-originated content should be removed immediately—without waiting for discussion, or for someone else to resolve the tagged issue.

If removal as described above would result in deletion of the entire contents of the article or draft, it then becomes a candidate for deletion.[c] If the entire page appears to be factually incorrect or relies on fabricated sources, speedy deletion per WP:G3 (Pure vandalism and blatant hoaxes) may be appropriate; if the entire page is obviously LLM-generated yet does not qualify for speedy deletion under G3, an alternative is to nominate the page for speedy deletion under the WP:G15 criterion.

On talk pages, apply the templates {{Collapse AI top}} and {{Collapse AI bottom}} to collapse discussions that are disruptive due to the use of LLM-generated text.

The following templates can be used to warn editors on their user talk pages:

{{uw-ai1}}
{{uw-ai2}}
{{uw-ai3}}
{{uw-ai4}}
The following templates can be used to nominate obviously LLM-generated articles for speedy deletion:

{{db-g15}}
{{db-llm}}
{{db-ai}}
See also
Wikipedia:WikiProject AI Cleanup, a group of editors focusing on the issue of non-policy-compliant LLM-originated content
Wikipedia:Artificial intelligence, an information page about the use of artificial intelligence on Wikipedia and Wikimedia projects
Wikipedia:Computer-generated content, a draft of a proposed policy on using computer-generated content in general on Wikipedia
Wikipedia:Using neural network language models on Wikipedia, an essay about large language models specifically
Artwork title, a surviving article initially developed from raw LLM output (before this page had been developed)
m:Research:Implications of ChatGPT for knowledge integrity on Wikipedia, an ongoing (as of July 2023) Wikimedia research project
Demonstrations
User:JPxG/LLM demonstration (wikitext markup, table rotation, reference analysis, article improvement suggestions, plot summarization, reference- and infobox-based expansion, proseline repair, uncited text tagging, table formatting and color schemes)
User:JPxG/LLM demonstration 2 (suggestions for article improvement, explanations of unclear maintenance templates based on article text)
User:Fuzheado/ChatGPT (PyWikiBot code, writing from scratch, Wikidata parsing, CSV parsing)
User:DraconicDark/ChatGPT (lead expansion)
Wikipedia:Using neural network language models on Wikipedia/Transcripts (showcases several actual mainspace LLM-assisted copyedits)
User:WeatherWriter/LLM Experiment 1 (identifying sourced and unsourced information)
User:WeatherWriter/LLM Experiment 2 (identifying sourced and unsourced information, including a non-English source)
User:WeatherWriter/LLM Experiment 3 (identifying sourced and unsourced information, only six of seven tests successful)
Wikipedia:Articles for deletion/ChatGPT and Wikipedia:Articles for deletion/Planet of the Apes (humorous April Fools' nominations generated almost entirely by large language models).
Wikipedia:Village pump (idea lab)/Archive 64 (demonstration of AI hallucination by Cremastra in response to a proposal for "Chatbot validation" of sentences in articles)
Policy discussions
This section is an excerpt from Wikipedia:Artificial intelligence § Discussion timeline.[edit]
Date	Type	Page	Discussion	Conclusion/Notes
Dec 2022		Wikipedia:Village pump (policy)	Wikipedia response to chatbot-generated content	
Feb 2023		Wikipedia:Village pump (idea lab)	OpenAI and ChatGPT	Disclosure suggested
Mar 2023		Wikipedia:Village pump (idea lab)	Adding LLM edit tag	Impractical with current technology
June 2023		Wikipedia:Village pump (miscellaneous)	GPT-4 user-created template at top of page	
Oct 2023	RfC	Wikipedia talk:Large language models	RfC: Is this proposal ready to be promoted?	Overwhelming consensus to not promote.
Oct 2023		Wikipedia:Village pump (idea lab)	Project Res-Up	About using AI to increase resolution on old photos
Nov 2023		Wikipedia:Village pump (proposals)	Scoring for Wikipedia type Articles Generated by LLM	External research project hoping to recruit Wikipedia editors for off-wiki feedback (not editing here)
Jan 2024	RfC	Wikipedia talk:Large language model policy	RFC	No consensus to adopt any wording as either a policy or guideline at this time.
Jan 2024		Wikipedia:Village pump (idea lab)	Can Wikipedia Provide An AI Tool To Evaluate News and Information on the Internet	
Jan 2024		Wikipedia:Village pump (idea lab)	Use of ChatGPT and other LLMs specifically for medical and scientific content	For text, not photos
Feb 2024		Wikipedia:Village pump (idea lab)	Have a way to prevent "hallucinated" AI-generated citations in articles	Goal supported in theory
March 2024		Wikipedia:Village pump (technical)	AI helper	Tool idea for creating articles
March 2024		Wikipedia:Village pump (technical)	What if we had an AI to suggest edits along the lines of edits typically made by good editors?	Tool idea for smaller edits
March 2024		Wikipedia:Village pump (proposals)	AI for WP guidelines/ policies	AI-based search of Wikipedia's ruleset
May 2024		Wikipedia:Village pump (idea lab)	Another job aid proposal, this time with AI	
Aug 2024		Wikipedia:Village pump (proposals)	Proposal: Create quizzes on Wikipedia	AI not seen as integral to the idea
Oct 2024		Wikipedia:Village pump (miscellaneous)	Feedback on chatbots as valid sources, or identifiers of them	
October 2024		Module talk:Find sources	Chatbots as valid sources or identifiers of them	Not supported at this time
Nov 2024		Wikipedia:Village pump (proposals)	Add AI translation option for translating from English to non-English article.	Off topic, as we don't decide what happens to other Wikipedias
Nov 2024		Wikipedia:Village pump (idea lab)	Wiki AI?	Request for a chatbot
Dec 2024	RfC	Wikipedia:Village pump (policy)	LLM/chatbot comments in discussions	Consensus that "it is within admins' and closers' discretion to discount, strike, or collapse obvious use of generative LLMs" (Now in guideline: WP:AITALK)
Jan 2025		Wikipedia:Village pump (proposals)	The use of AI-generated content	Proposed rule accepting LLMs for translation and grammar but not on talk pages; not accepted
Jan 2025	RfC	Wikipedia:Village pump (policy)	Wikipedia:Requests for comment/AI images#BLPs	Clear consensus against using AI-generated imagery to depict BLP subjects. (Now in policy: WP:AIIMGBLP)
Jan 2025		Wikipedia:Village pump (policy)	Adding the undisclosed use of AI to post a wall of text into discussions as disruptive editing	Not inherently disruptive, but can be disruptive
Feb 2025		Wikipedia:Village pump (policy)	The real use case for AI on Wikipedia	Ideas for copyediting and grammar fixes
March 2025		Wikipedia:Village pump (policy)	URLs with utm_source=chatgpt.com codes	
April 2025	RfC	Wikipedia:Village pump (policy)	Wikipedia:Requests for comment/AI images#Relist with broader question: Ban all AI images?	"Most images wholly generated by AI should not be used." "Obvious exceptions include articles about AI, and articles about notable AI-generated images. The community objects particularly strongly to AI-generated images (1) of named people, and (2) in technical or scientific subjects such as anatomy and chemistry." (Now in policy: WP:AIIMAGES)
June 2025	RfC	Wikipedia:Village pump (WMF)	Wikipedia:Village pump (WMF)/Archive 10#RfC: Adopting a community position on WMF AI development	
June 2025		Wikipedia:Village pump (technical)	Wikipedia:Village_pump (technical)#Simple summaries: editor survey and 2-week mobile study	
Notes
 This also applies to cases in which the AI model is in a jurisdiction where works generated solely by AI is not copyrightable, although with very low probability.
 For example, someone skilled at dealing with vandalism but doing very little article work should probably not start creating articles using LLMs. Instead, they should first gather actual experience at article creation without the assistance of the LLM.
 Whenever a new article largely consists of unedited output of a large language model, it may be draftified, per WP:DRAFTREASON.
As long as the title indicates a topic that has some potential merit, it may be worth it to stubify or blank-and-redirect. Likewise, drafts about viable new topics may be convertible to "skeleton drafts", i.e. near-blanked, by leaving only a brief definition of the subject. Creators of such pages should be suitably notified or warned. Whenever suspected LLM-generated content is concerned, editors are discouraged from contesting instances of removal through reversal without discussing first.
When an alternative to deletion is considered, editors should still be mindful of any outstanding copyright or similar critical issues which would necessitate deletion.
References
 Smith, Adam (25 January 2023). "What Is ChatGPT? And Will It Steal Our Jobs?". Context. Thomson Reuters Foundation. Retrieved 27 January 2023.
 "When AI Gets It Wrong: Addressing AI Hallucinations and Bias". MIT Sloan Teaching & Learning Technologies. Retrieved 2025-05-25.
vte
Wikipedia essays (?)
Essays on building, editing, and deleting content
Essays on civility
Essays on notability
Humorous essays
About essays
Categories: Wikipedia essaysWikipedia and artificial intelligence